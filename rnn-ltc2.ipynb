{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92023b2-5dd2-408a-8106-97845b6c1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Reshape, BatchNormalization, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from ncps.tf import LTC\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Mounting Google Drive... Done.\")\n",
    "\n",
    "# Function to scale input data\n",
    "def scaled_in(matrix_spec):\n",
    "    \"Global scaling applied to noisy voice spectrograms (scale between -1 and 1)\"\n",
    "    matrix_spec = (matrix_spec + 46) / 50\n",
    "    return matrix_spec\n",
    "\n",
    "print(\"Defining scaled_in function... Done.\")\n",
    "\n",
    "# Function to scale output data\n",
    "def scaled_ou(matrix_spec):\n",
    "    \"Global scaling applied to noise models spectrograms (scale between -1 and 1)\"\n",
    "    matrix_spec = (matrix_spec - 6) / 82\n",
    "    return matrix_spec\n",
    "\n",
    "print(\"Defining scaled_ou function... Done.\")\n",
    "\n",
    "# Load data\n",
    "path_save_spectrogram = '/content/drive/MyDrive/npy/'  # Specify the path to the spectrogram directory\n",
    "\n",
    "print(\"Loading data...\")\n",
    "X_in = np.load(os.path.join(path_save_spectrogram, 'noisy_voice_amp_db.npy'))\n",
    "X_ou = np.load(os.path.join(path_save_spectrogram, 'voice_amp_db.npy'))\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Model of noise to predict\n",
    "X_ou = X_in - X_ou\n",
    "\n",
    "# Check distribution\n",
    "print(\"Checking distribution of input data...\")\n",
    "print(stats.describe(X_in.reshape(-1, 1)))\n",
    "print(stats.describe(X_ou.reshape(-1, 1)))\n",
    "print(\"Distribution check completed.\")\n",
    "\n",
    "# Scale input and output data\n",
    "print(\"Scaling input and output data...\")\n",
    "X_in = scaled_in(X_in)\n",
    "X_ou = scaled_ou(X_ou)\n",
    "print(\"Data scaled successfully.\")\n",
    "\n",
    "# Check shape of spectrograms\n",
    "print(\"Checking shape of spectrograms...\")\n",
    "print(\"Input shape:\", X_in.shape)\n",
    "print(\"Output shape:\", X_ou.shape)\n",
    "print(\"Shape check completed.\")\n",
    "\n",
    "# Reshape for training\n",
    "print(\"Reshaping data for training...\")\n",
    "X_in = X_in[:, :, :]\n",
    "X_in = X_in.reshape(X_in.shape[0], X_in.shape[1], X_in.shape[2], 1)\n",
    "X_ou = X_ou[:, :, :]\n",
    "X_ou = X_ou.reshape(X_ou.shape[0], X_ou.shape[1], X_ou.shape[2], 1)\n",
    "print(\"Data reshaped successfully.\")\n",
    "\n",
    "# Split data into train and validation sets\n",
    "print(\"Splitting data into train and validation sets...\")\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_in, X_ou, test_size=0.10, random_state=42)\n",
    "print(\"Data split completed.\")\n",
    "\n",
    "# Load the saved UNet model\n",
    "# No UNet model in this case, since we're only using RNNs\n",
    "\n",
    "# Define RNN model\n",
    "print(\"Defining RNN model...\")\n",
    "ncp = LTC(32, 16)\n",
    "rnn_model = tf.keras.Sequential()\n",
    "rnn_model.add(Reshape((-1, 1)))  # Reshape to add a time dimension\n",
    "rnn_model.add(ncp)\n",
    "rnn_model.add(BatchNormalization())\n",
    "rnn_model.add(Dense(128, activation='relu'))  # Adjust output size to match input size\n",
    "rnn_model.add(Dense(128, activation='relu'))  # Adjust output size to match input size\n",
    "rnn_model.add(Dense(128, activation='relu'))  # Adjust output size to match input size\n",
    "rnn_model.add(Dense(1, activation='linear'))  # Adjust output size to match input size\n",
    "print(\"RNN model defined successfully.\")\n",
    "\n",
    "# Compile the RNN model\n",
    "print(\"Compiling the RNN model...\")\n",
    "rnn_model.compile(optimizer='Adam',\n",
    "                  loss='mean_squared_error',  # Use mean squared error loss\n",
    "                  metrics=['mae'])\n",
    "print(\"RNN model compiled successfully.\")\n",
    "\n",
    "# Define filepath for saving the best model\n",
    "model_checkpoint_path = 'best_model_rnn.keras'\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint = ModelCheckpoint(model_checkpoint_path,\n",
    "                             verbose=1,\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             mode='auto')\n",
    "\n",
    "# Train the RNN model\n",
    "print(\"Training the RNN model...\")\n",
    "history = rnn_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=20, batch_size=10, verbose=1, callbacks=[checkpoint])\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Save model architecture to JSON file\n",
    "print(\"Saving model architecture to JSON file...\")\n",
    "model_json = rnn_model.to_json()\n",
    "with open('model_rnn.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Model architecture saved successfully.\")\n",
    "\n",
    "# Save model weights\n",
    "print(\"Saving model weights...\")\n",
    "rnn_model.save_weights('model_rnn_weights.keras')\n",
    "print(\"Model weights saved successfully.\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print epoch information and other variables\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Training Loss:\", loss)\n",
    "print(\"Validation Loss:\", val_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
