{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6472cc7b-e683-41de-905b-aba016cc15cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_24_input (InputLaye  [(None, 128, 128, 1)]     0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 128, 128, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 64, 64, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 32, 32, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 16, 16, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 32768)             0         \n",
      "                                                                 \n",
      " sequential_8 (Sequential)   (None, 11)                4207435   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4493867 (17.14 MB)\n",
      "Trainable params: 4493803 (17.14 MB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Reshape, BatchNormalization, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from ncps.tf import LTC\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Function to scale input data\n",
    "def scaled_in(matrix_spec):\n",
    "    \"Global scaling applied to noisy voice spectrograms (scale between -1 and 1)\"\n",
    "    matrix_spec = (matrix_spec + 46) / 50\n",
    "    return matrix_spec\n",
    "\n",
    "# Function to scale output data\n",
    "def scaled_ou(matrix_spec):\n",
    "    \"Global scaling applied to noise models spectrograms (scale between -1 and 1)\"\n",
    "    matrix_spec = (matrix_spec - 6) / 82\n",
    "    return matrix_spec\n",
    "\n",
    "# Load data\n",
    "path_save_spectrogram = '/content/drive/MyDrive/npy/'  # Specify the path to the spectrogram directory\n",
    "X_in = np.load(os.path.join(path_save_spectrogram, 'noisy_voice_amp_db.npy'))\n",
    "X_ou = np.load(os.path.join(path_save_spectrogram, 'voice_amp_db.npy'))\n",
    "\n",
    "# Model of noise to predict\n",
    "X_ou = X_in - X_ou\n",
    "\n",
    "# Check distribution\n",
    "print(stats.describe(X_in.reshape(-1, 1)))\n",
    "print(stats.describe(X_ou.reshape(-1, 1)))\n",
    "\n",
    "# Scale input and output data\n",
    "X_in = scaled_in(X_in)\n",
    "X_ou = scaled_ou(X_ou)\n",
    "\n",
    "# Check shape of spectrograms\n",
    "print(X_in.shape)\n",
    "print(X_ou.shape)\n",
    "# Check new distribution\n",
    "print(stats.describe(X_in.reshape(-1, 1)))\n",
    "print(stats.describe(X_ou.reshape(-1, 1)))\n",
    "\n",
    "# Reshape for training\n",
    "X_in = X_in[:, :, :]\n",
    "X_in = X_in.reshape(X_in.shape[0], X_in.shape[1], X_in.shape[2], 1)\n",
    "X_ou = X_ou[:, :, :]\n",
    "X_ou = X_ou.reshape(X_ou.shape[0], X_ou.shape[1], X_ou.shape[2], 1)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_in, X_ou, test_size=0.10, random_state=42)\n",
    "\n",
    "# Load the saved UNet model\n",
    "unet_model = load_model(os.path.join(path_save_spectrogram, 'weights/model_ResNet.keras'))\n",
    "\n",
    "# Define RNN model\n",
    "ncp = LTC(32, 16)\n",
    "rnn_model = tf.keras.Sequential()\n",
    "rnn_model.add(Reshape((-1, 1)))  # Reshape to add a time dimension\n",
    "rnn_model.add(ncp)\n",
    "rnn_model.add(BatchNormalization())\n",
    "rnn_model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "# Define the combined model\n",
    "combined_output = rnn_model(unet_model.output)\n",
    "combined_model = Model(inputs=unet_model.input, outputs=combined_output)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(optimizer='Adam',\n",
    "                       loss=tf.keras.losses.MeanSquaredError(),\n",
    "                       metrics=['mae'])\n",
    "\n",
    "# Define filepath for saving the best model\n",
    "model_checkpoint_path = 'best_model_combined.keras'\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint = ModelCheckpoint(model_checkpoint_path,\n",
    "                             verbose=1,\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             mode='auto')\n",
    "\n",
    "# Train the combined model\n",
    "history = combined_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=20, batch_size=10, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "# Save model architecture to JSON file\n",
    "model_json = combined_model.to_json()\n",
    "with open('model_combined.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save model weights\n",
    "combined_model.save_weights('model_combined_weights.keras')\n",
    "\n",
    "# Plot training and validation loss (log scale)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print epoch information and other variables\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Training Loss:\", loss)\n",
    "print(\"Validation Loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0ad72-a163-4ac5-a7df-b7d39901b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_cnn(path_save_spectrogram, weights_path, epochs, batch_size):\n",
    "    # Load noisy voice & clean voice spectrograms created by data_creation mode\n",
    "    X_in = np.load(path_save_spectrogram +'noisy_voice_amp_db'+\".npy\")\n",
    "    X_ou = np.load(path_save_spectrogram +'voice_amp_db'+\".npy\")\n",
    "\n",
    "    # Model of noise to predict\n",
    "    X_ou = X_in - X_ou\n",
    "\n",
    "    # Check distribution\n",
    "    print(stats.describe(X_in.reshape(-1,1)))\n",
    "    print(stats.describe(X_ou.reshape(-1,1)))\n",
    "\n",
    "    # Rescale between -1 and 1\n",
    "    X_in = scaled_in(X_in)\n",
    "    X_ou = scaled_ou(X_ou)\n",
    "\n",
    "    # Check shape of spectrograms\n",
    "    print(X_in.shape)\n",
    "    print(X_ou.shape)\n",
    "    # Check new distribution\n",
    "    print(stats.describe(X_in.reshape(-1,1)))\n",
    "    print(stats.describe(X_ou.reshape(-1,1)))\n",
    "\n",
    "    # Reshape for training\n",
    "    X_in = X_in[:, :, :]\n",
    "    X_in = X_in.reshape(X_in.shape[0], X_in.shape[1], X_in.shape[2], 1)\n",
    "    X_ou = X_ou[:, :, :]\n",
    "    X_ou = X_ou.reshape(X_ou.shape[0], X_ou.shape[1], X_ou.shape[2], 1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_in, X_ou, test_size=0.10, random_state=42)\n",
    "    \n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "    # Create CNN model\n",
    "    model = cnn_model(X_train.shape[1:])\n",
    "\n",
    "    # Save best models to disk during training\n",
    "    checkpoint = ModelCheckpoint(weights_path + '/model_cnn_best.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto')\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Training\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, shuffle=True, callbacks=[checkpoint], verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Saving Model\n",
    "    model.save_weights(weights_path + '/model_cnn_final.h5')\n",
    "\n",
    "    # Plot training and validation loss (log scale)\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss, label='Training loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation loss')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have the necessary data files and directories set up\n",
    "\n",
    "# Set the paths and parameters\n",
    "path_save_spectrogram = 'spectogram/'\n",
    "weights_path = 'weights/'\n",
    "epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "# Call the training function\n",
    "training_cnn(path_save_spectrogram, weights_path, epochs, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
